import torch
import torch.nn as nn
import torch.nn.functional as F

# ==========================================================
# ðŸ§© ç®€å• MLP ç¼–ç å™¨
# ==========================================================
class MLPEncoder(nn.Module):
    def __init__(self, input_dim=3072, hidden_dim=512, dropout=0.1, init_std=0.02):
        super().__init__()
        self.fc = nn.Linear(input_dim, hidden_dim)
        self.act = nn.ReLU()
        self.norm = nn.LayerNorm(hidden_dim)
        self.drop = nn.Dropout(dropout)

        # âœ… Gaussian åˆå§‹åŒ–ï¼ˆä¿æŒä»»åŠ¡é—´åˆ†å¸ƒä¸€è‡´ï¼‰
        nn.init.normal_(self.fc.weight, mean=0.0, std=init_std)
        if self.fc.bias is not None:
            nn.init.zeros_(self.fc.bias)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.drop(self.norm(self.act(self.fc(x))))
        return x


# ==========================================================
# ðŸŽ¯ å¤šä»»åŠ¡æ¨¡åž‹ï¼ˆæ”¯æŒ task-specific Ï„áµ¢ ä¸Žä¸ç¡®å®šæ€§åŠ æƒ UWï¼‰
# ==========================================================
class MultiTaskModel(nn.Module):
    def __init__(self, cfg_model, use_uw=False):
        super().__init__()

        # ---------------- Encoder & Head ----------------
        enc = cfg_model["encoder"]
        head = cfg_model["heads"]

        self.encoder = MLPEncoder(enc["input_dim"], enc["hidden_dim"], enc["dropout"])
        self.num_tasks = head["num_tasks"]
        self.out_dim = head["out_dim"]

        # å¤šä»»åŠ¡åˆ†ç±»å¤´
        self.heads = nn.ModuleList([
            nn.Linear(enc["hidden_dim"], self.out_dim)
            for _ in range(self.num_tasks)
        ])

        # âœ… åˆå§‹åŒ– heads æƒé‡
        for h in self.heads:
            nn.init.normal_(h.weight, mean=0.0, std=0.02)
            if h.bias is not None:
                nn.init.zeros_(h.bias)

        # ---------------- Task-specific Ï„ å‚æ•° ----------------
        self.log_taus = nn.Parameter(torch.log(torch.ones(self.num_tasks) * 2.5))

        # ---------------- UW å‚æ•°ï¼ˆå¯é€‰ï¼‰ ----------------
        self.use_uw = use_uw
        if use_uw:
            self.log_vars = nn.Parameter(torch.zeros(self.num_tasks))
        else:
            self.register_buffer("log_vars", torch.zeros(self.num_tasks))

        # ---------------- å¯é€‰æ­£åˆ™å‚æ•° Î»â‚ ----------------
        self.lambda_a = head.get("lambda_a", 0.0)

    # ======================================================
    # ðŸ”¹ å‰å‘ä¼ æ’­
    # ======================================================
    def forward(self, x, task_idx):
        """
        è¾“å…¥:
            x: Tensor [B, ...]
            task_idx: å½“å‰ä»»åŠ¡ç´¢å¼• (int)
        è¾“å‡º:
            logits: å½“å‰ä»»åŠ¡è¾“å‡º
            h: å…±äº«ç¼–ç ç‰¹å¾
        """
        h = self.encoder(x)
        logits = self.heads[task_idx](h)
        return logits, h

    # ======================================================
    # ðŸ”¹ ä»»åŠ¡ä¸“å±ž Loss è®¡ç®—
    # ======================================================
    def task_loss(self, logits, y, t):
        """
        æ¯ä»»åŠ¡ä½¿ç”¨è‡ªå·±çš„ Ï„áµ¢:
            Ï„áµ¢ = exp(log_Ï„áµ¢)
        è‹¥å¯ç”¨ UW:
            L_t = 0.5 * exp(-s_t) * CE + 0.5 * s_t
        """
        # --- æ¸©åº¦ç¼©æ”¾ ---
        tau = torch.exp(self.log_taus[t]).clamp(0.5, 3.0)
        scaled_logits = logits / tau
        ce = F.cross_entropy(scaled_logits, y)

        # --- ä¸ç¡®å®šæ€§åŠ æƒ UW ---
        if self.use_uw:
            # âœ… é™åˆ¶ log_var é¿å…æº¢å‡º
            clamped_log_var = torch.clamp(self.log_vars[t], min=-5.0, max=5.0)
            precision = torch.exp(-clamped_log_var)
            loss = 0.5 * precision * ce + 0.5 * clamped_log_var
        else:
            loss = ce

        # --- å¯é€‰ä»»åŠ¡å¤´æ­£åˆ™åŒ– ---
        if self.lambda_a > 0:
            reg = 0.0
            for p in self.heads[t].parameters():
                reg += torch.sum(p ** 2)
            loss = loss + self.lambda_a * reg

        return loss

